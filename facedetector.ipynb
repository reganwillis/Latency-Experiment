{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10984842,"sourceType":"datasetVersion","datasetId":6836738}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# packages\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import models, transforms\nfrom torchmetrics.detection import IntersectionOverUnion\nfrom PIL import Image\nimport os\nimport cv2\nimport numpy as np\nimport json\nimport shutil\nfrom torchsummary import summary\nfrom PIL import Image\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:20:20.577034Z","iopub.execute_input":"2025-04-03T15:20:20.577332Z","iopub.status.idle":"2025-04-03T15:20:31.073619Z","shell.execute_reply.started":"2025-04-03T15:20:20.577302Z","shell.execute_reply":"2025-04-03T15:20:31.072954Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#constants\nDEBUG = True\nBATCH_SIZE = 32\nNUM_EPOCHS = 3\n\nif not torch.cuda.is_available():\n    print('GPU not available, running script on CPU..')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:20:31.074393Z","iopub.execute_input":"2025-04-03T15:20:31.074716Z","iopub.status.idle":"2025-04-03T15:20:31.122466Z","shell.execute_reply.started":"2025-04-03T15:20:31.074697Z","shell.execute_reply":"2025-04-03T15:20:31.121536Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#Data base addresses\nbase_adress = '/kaggle/input/Dataset_FDDB/Dataset_FDDB/images'\nlabels_adr = '/kaggle/input/Dataset_FDDB/Dataset_FDDB/label.txt'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:20:31.123410Z","iopub.execute_input":"2025-04-03T15:20:31.123671Z","iopub.status.idle":"2025-04-03T15:20:31.148041Z","shell.execute_reply.started":"2025-04-03T15:20:31.123638Z","shell.execute_reply":"2025-04-03T15:20:31.147286Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Make the labels ready\n#import os\n#print(os.listdir('/kaggle/input/Dataset_FDDB/Dataset_FDDB/label.txt'))\nwith open(labels_adr, 'r') as f:\n    lines = f.readlines()\nannotations = []\nbboxes = []\nflag = False\nfor line in lines:\n    if line.startswith('#'):\n      if flag:\n        annotations.append({'image':img_name, 'bboxes': bboxes})\n        bboxes = []\n      flag = True\n      img_name = line[2:]\n    else:\n      x_min, y_min, x_max, y_max = line.split()\n      bboxes.append([int(x_min), int(y_min), int(x_max), int(y_max)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:20:31.149975Z","iopub.execute_input":"2025-04-03T15:20:31.150206Z","iopub.status.idle":"2025-04-03T15:20:31.191637Z","shell.execute_reply.started":"2025-04-03T15:20:31.150180Z","shell.execute_reply":"2025-04-03T15:20:31.191054Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Custom Dataset Class for FDDB\nclass FDDBDataset(Dataset):\n    def __init__(self, img_dir, annot_file, target_size=(224, 224), _transform=None, augment=False):\n        self.img_dir = img_dir\n        self.target_size = target_size\n        self.augment = augment\n        self.data = self._parse_annotations(annot_file)\n        self._transform = _transform\n\n    def _parse_annotations(self, annot_file):\n        \n        data = []\n        for el in annot_file:\n          img_path = os.path.join(self.img_dir, el['image'][:-1])\n          boxes = el['bboxes']\n          data.append((img_path, boxes))\n        return data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_path, boxes = self.data[idx]\n        image = cv2.imread(img_path)\n        if image is None:\n            raise FileNotFoundError(f\"Image not found: {img_path}\")\n\n        # Original dimensions\n        h, w, _ = image.shape\n\n        # Resize image\n        image_resized = cv2.resize(image, self.target_size)\n        target_h, target_w = self.target_size\n\n        # Scale bounding boxes\n        scale_x = target_w / w\n        scale_y = target_h / h\n        boxes_resized = []\n        for box in boxes:\n            x_min = int(box[0] * scale_x)\n            y_min = int(box[1] * scale_y)\n            x_max = int(box[2] * scale_x)\n            y_max = int(box[3] * scale_y)\n            boxes_resized.append([x_min, y_min, x_max, y_max])\n\n        # Convert to tensor\n        if self.augment:\n            image_resized = self.transform(image_resized)\n        else:\n            #image_resized = transforms.ToTensor()(image_resized)\n            image_resized = self._transform(image_resized)\n\n        return image_resized, torch.tensor(boxes_resized, dtype=torch.float32)\n    \n    def transform(self, image):\n        # data augmentation\n        image_array = np.array(image)\n        image = Image.fromarray(image_array)\n\n        # random scaling up to 20% original image size\n        #scale = transforms.RandomAffine(degrees=0, scale=(0.2, 1.2))\n        #image = scale(image)\n\n        # random translations up to 20% original image size\n        #translate = transforms.RandomAffine(degrees=0, translate=(0.2, 0.2))\n        #image = translate(image)\n\n        # randomly adjust exposure of image by factor of 1.5\n        #adjust_exposure = transforms.ColorJitter(0, 1.5)\n        #image = adjust_exposure(image)\n\n        # randomly adjust saturation of image by factor of 1.5\n        #adjust_saturation = transforms.ColorJitter(0, 1.5)\n        #image = adjust_saturation(image)\n        \n        to_tensor = transforms.ToTensor()\n        image = to_tensor(image)\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        image = normalize(image)\n\n        return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:20:31.192913Z","iopub.execute_input":"2025-04-03T15:20:31.193244Z","iopub.status.idle":"2025-04-03T15:20:31.201494Z","shell.execute_reply.started":"2025-04-03T15:20:31.193223Z","shell.execute_reply":"2025-04-03T15:20:31.200654Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# DataLoader preparation\ndef get_dataloaders(img_dir, annot_file, batch_size=BATCH_SIZE, target_size=(224, 224), validation_split=0.2):\n    _transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    \n    # Dataset\n    dataset = FDDBDataset(img_dir, annot_file, target_size, _transform, augment=False)\n\n    # Split dataset\n    val_size = int(len(dataset) * validation_split)\n    train_size = len(dataset) - val_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n    # DataLoaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\n    return train_loader, val_loader\n\ndef collate_fn(batch):\n    \"\"\"\n    Custom collate function to handle variable-length bounding box arrays.\n\n    :param batch: List of tuples (image, boxes).\n    :return: Tuple of images and targets.\n    \"\"\"\n    images = torch.stack([item[0] for item in batch])\n    targets = [item[1] for item in batch]\n    return images, targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:20:31.202289Z","iopub.execute_input":"2025-04-03T15:20:31.202588Z","iopub.status.idle":"2025-04-03T15:20:31.222383Z","shell.execute_reply.started":"2025-04-03T15:20:31.202560Z","shell.execute_reply":"2025-04-03T15:20:31.221729Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class MyFaceDetector(nn.Module):\n    def __init__(self, pretrained=False):\n        super().__init__()\n        # TODO: resize input images to 448p (or stay 244p?)\n        # TODO: threshold the resulting detections by the model's confidence\n        # TODO: feature extractor backbone\n        # architecture from original YOLO paper\n\n        # conv layer 1\n        self.conv1 = nn.Conv2d(\n            in_channels=3,\n            out_channels=64,\n            kernel_size=7,\n            stride=2,\n            padding=1\n        )\n        self.relu1 = nn.LeakyReLU(negative_slope=0.1)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv2 = nn.Conv2d(\n            in_channels=64,\n            out_channels=192,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu2 = nn.LeakyReLU(negative_slope=0.1)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv3 = nn.Conv2d(\n            in_channels=192,\n            out_channels=128,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu3 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv4 = nn.Conv2d(\n            in_channels=128,\n            out_channels=256,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu4 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv5 = nn.Conv2d(\n            in_channels=256,\n            out_channels=256,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu5 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv6 = nn.Conv2d(\n            in_channels=256,\n            out_channels=512,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu6 = nn.LeakyReLU(negative_slope=0.1)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv7 = nn.Conv2d(\n            in_channels=512,\n            out_channels=256,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu7 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv8 = nn.Conv2d(\n            in_channels=256,\n            out_channels=512,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu8 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv9 = nn.Conv2d(\n            in_channels=512,\n            out_channels=256,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu9 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv10 = nn.Conv2d(\n            in_channels=256,\n            out_channels=512,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu10 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv11 = nn.Conv2d(\n            in_channels=512,\n            out_channels=256,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu11 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv12 = nn.Conv2d(\n            in_channels=256,\n            out_channels=512,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu12 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv13 = nn.Conv2d(\n            in_channels=512,\n            out_channels=256,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu13 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv14 = nn.Conv2d(\n            in_channels=256,\n            out_channels=512,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu14 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv15 = nn.Conv2d(\n            in_channels=512,\n            out_channels=512,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu15 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv16 = nn.Conv2d(\n            in_channels=512,\n            out_channels=1024,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu16 = nn.LeakyReLU(negative_slope=0.1)\n        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.conv17 = nn.Conv2d(\n            in_channels=1024,\n            out_channels=512,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu17 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv18 = nn.Conv2d(\n            in_channels=512,\n            out_channels=1024,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu18 = nn.LeakyReLU(negative_slope=0.1)\n        # TODO: is the order of multipliers from the architecture correct?\n        self.conv19 = nn.Conv2d(\n            in_channels=1024,\n            out_channels=512,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu19 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv20 = nn.Conv2d(\n            in_channels=512,\n            out_channels=1024,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu20 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv21 = nn.Conv2d(\n            in_channels=1024,\n            out_channels=1024,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu21 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv22 = nn.Conv2d(\n            in_channels=1024,\n            out_channels=1024,\n            kernel_size=3,\n            stride=2,\n            padding=1\n        )\n        self.relu22 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv23 = nn.Conv2d(\n            in_channels=1024,\n            out_channels=1024,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu23 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv24 = nn.Conv2d(\n            in_channels=1024,\n            out_channels=1024,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu24 = nn.LeakyReLU(negative_slope=0.1)\n\n        self.flatten = nn.Flatten()\n\n        # regression head\n        self.bbox = nn.Sequential(\n            nn.Linear(1024*8*8, 4096),\n            nn.LeakyReLU(negative_slope=0.1),\n            nn.Dropout(p=0.5),\n            nn.Linear(4096, 4)\n        )\n        \n        # classification head\n        self.classify = nn.Sequential(\n            nn.Linear(1024*8*8, 4096),\n            nn.LeakyReLU(negative_slope=0.1),\n            nn.Dropout(p=0.5),\n            nn.Linear(4096, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        # pass through feature extractor backbone\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.maxpool1(x)\n        x = self.conv2(x)\n        x = self.relu2(x)\n        x = self.maxpool2(x)\n        x = self.conv3(x)\n        x = self.relu3(x)\n        x = self.conv4(x)\n        x = self.relu4(x)\n        x = self.conv5(x)\n        x = self.relu5(x)\n        x = self.conv6(x)\n        x = self.relu6(x)\n        x = self.maxpool3(x)\n        x = self.conv7(x)\n        x = self.relu7(x)\n        x = self.conv8(x)\n        x = self.relu8(x)\n        x = self.conv9(x)\n        x = self.relu9(x)\n        x = self.conv10(x)\n        x = self.relu10(x)\n        x = self.conv11(x)\n        x = self.relu11(x)\n        x = self.conv12(x)\n        x = self.relu12(x)\n        x = self.conv13(x)\n        x = self.relu13(x)\n        x = self.conv14(x)\n        x = self.relu14(x)\n        x = self.conv15(x)\n        x = self.relu15(x)\n        x = self.conv16(x)\n        x = self.relu16(x)\n        x = self.maxpool4(x)\n        x = self.conv17(x)\n        x = self.relu17(x)\n        x = self.conv18(x)\n        x = self.relu18(x)\n        x = self.conv19(x)\n        x = self.relu19(x)\n        x = self.conv20(x)\n        x = self.relu20(x)\n        x = self.conv21(x)\n        x = self.relu21(x)\n        x = self.conv22(x)\n        x = self.relu22(x)\n        x = self.conv23(x)\n        x = self.relu23(x)\n        x = self.conv24(x)\n        x = self.relu24(x)\n\n        #_, c, h, w = x.shape\n        #print(c, h, w)\n        \n        x = self.flatten(x)\n        \n        bbox = self.bbox(x)\n        label = self.classify(x)\n\n        return bbox, label\nmodel = MyFaceDetector().to(device)\nsummary(model, (3, 224, 224))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:20:31.222989Z","iopub.execute_input":"2025-04-03T15:20:31.223244Z","iopub.status.idle":"2025-04-03T15:20:37.849118Z","shell.execute_reply.started":"2025-04-03T15:20:31.223224Z","shell.execute_reply":"2025-04-03T15:20:37.848244Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 110, 110]           9,472\n         LeakyReLU-2         [-1, 64, 110, 110]               0\n         MaxPool2d-3           [-1, 64, 55, 55]               0\n            Conv2d-4          [-1, 192, 55, 55]         110,784\n         LeakyReLU-5          [-1, 192, 55, 55]               0\n         MaxPool2d-6          [-1, 192, 27, 27]               0\n            Conv2d-7          [-1, 128, 29, 29]          24,704\n         LeakyReLU-8          [-1, 128, 29, 29]               0\n            Conv2d-9          [-1, 256, 29, 29]         295,168\n        LeakyReLU-10          [-1, 256, 29, 29]               0\n           Conv2d-11          [-1, 256, 31, 31]          65,792\n        LeakyReLU-12          [-1, 256, 31, 31]               0\n           Conv2d-13          [-1, 512, 31, 31]       1,180,160\n        LeakyReLU-14          [-1, 512, 31, 31]               0\n        MaxPool2d-15          [-1, 512, 15, 15]               0\n           Conv2d-16          [-1, 256, 17, 17]         131,328\n        LeakyReLU-17          [-1, 256, 17, 17]               0\n           Conv2d-18          [-1, 512, 17, 17]       1,180,160\n        LeakyReLU-19          [-1, 512, 17, 17]               0\n           Conv2d-20          [-1, 256, 19, 19]         131,328\n        LeakyReLU-21          [-1, 256, 19, 19]               0\n           Conv2d-22          [-1, 512, 19, 19]       1,180,160\n        LeakyReLU-23          [-1, 512, 19, 19]               0\n           Conv2d-24          [-1, 256, 21, 21]         131,328\n        LeakyReLU-25          [-1, 256, 21, 21]               0\n           Conv2d-26          [-1, 512, 21, 21]       1,180,160\n        LeakyReLU-27          [-1, 512, 21, 21]               0\n           Conv2d-28          [-1, 256, 23, 23]         131,328\n        LeakyReLU-29          [-1, 256, 23, 23]               0\n           Conv2d-30          [-1, 512, 23, 23]       1,180,160\n        LeakyReLU-31          [-1, 512, 23, 23]               0\n           Conv2d-32          [-1, 512, 25, 25]         262,656\n        LeakyReLU-33          [-1, 512, 25, 25]               0\n           Conv2d-34         [-1, 1024, 25, 25]       4,719,616\n        LeakyReLU-35         [-1, 1024, 25, 25]               0\n        MaxPool2d-36         [-1, 1024, 12, 12]               0\n           Conv2d-37          [-1, 512, 14, 14]         524,800\n        LeakyReLU-38          [-1, 512, 14, 14]               0\n           Conv2d-39         [-1, 1024, 14, 14]       4,719,616\n        LeakyReLU-40         [-1, 1024, 14, 14]               0\n           Conv2d-41          [-1, 512, 16, 16]         524,800\n        LeakyReLU-42          [-1, 512, 16, 16]               0\n           Conv2d-43         [-1, 1024, 16, 16]       4,719,616\n        LeakyReLU-44         [-1, 1024, 16, 16]               0\n           Conv2d-45         [-1, 1024, 16, 16]       9,438,208\n        LeakyReLU-46         [-1, 1024, 16, 16]               0\n           Conv2d-47           [-1, 1024, 8, 8]       9,438,208\n        LeakyReLU-48           [-1, 1024, 8, 8]               0\n           Conv2d-49           [-1, 1024, 8, 8]       9,438,208\n        LeakyReLU-50           [-1, 1024, 8, 8]               0\n           Conv2d-51           [-1, 1024, 8, 8]       9,438,208\n        LeakyReLU-52           [-1, 1024, 8, 8]               0\n          Flatten-53                [-1, 65536]               0\n           Linear-54                 [-1, 4096]     268,439,552\n        LeakyReLU-55                 [-1, 4096]               0\n          Dropout-56                 [-1, 4096]               0\n           Linear-57                    [-1, 4]          16,388\n           Linear-58                 [-1, 4096]     268,439,552\n        LeakyReLU-59                 [-1, 4096]               0\n          Dropout-60                 [-1, 4096]               0\n           Linear-61                    [-1, 1]           4,097\n          Sigmoid-62                    [-1, 1]               0\n================================================================\nTotal params: 597,055,557\nTrainable params: 597,055,557\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 93.33\nParams size (MB): 2277.59\nEstimated Total Size (MB): 2371.49\n----------------------------------------------------------------\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"class MobileNetV2FaceDetector(nn.Module):\n    def __init__(self, pretrained=True):\n        super(MobileNetV2FaceDetector, self).__init__()\n        # Load MobileNetV2 base\n        self.base = models.mobilenet_v2(pretrained=pretrained).features\n        self.pool = nn.AdaptiveAvgPool2d(1)\n\n        # Custom head for bounding box and classification\n        self.fc_bbox = nn.Sequential(\n            nn.Linear(1280, 512),\n            nn.ReLU(),\n            nn.Linear(512, 4),  # Bounding box: [x_min, y_min, x_max, y_max]\n        )\n        self.fc_label = nn.Sequential(\n            nn.Linear(1280, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1),  # Binary classification: face/no face\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        x = self.base(x)\n        x = self.pool(x).view(x.size(0), -1)\n        bbox = self.fc_bbox(x)\n        label = self.fc_label(x)\n        return bbox, label\n\n#model = MobileNetV2FaceDetector().to(device)\n# after 100 epochs, IoU was ~0.56","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-04-03T15:20:37.850215Z","iopub.execute_input":"2025-04-03T15:20:37.850534Z","iopub.status.idle":"2025-04-03T15:20:37.856241Z","shell.execute_reply.started":"2025-04-03T15:20:37.850504Z","shell.execute_reply":"2025-04-03T15:20:37.855391Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# loss / optimizer / train\n# Loss functions\nbbox_loss_fn = nn.SmoothL1Loss()  # For bounding box regression\nlabel_loss_fn = nn.BCELoss()      # For binary classification\n\n# Optimizer\noptimizer = optim.Adam(model.parameters(), lr=1e-3)#, weight_decay=0.0005)\n#optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.0005)\n\n# 1e-2: 75 epochs\n# 1e-3: 30 epochs\n# 1e-4: 30 epochs\n#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n\n# Training loop\ndef train_model(model, train_loader, val_loader, num_epochs):\n    best_val_loss = float('inf')  # Initialize best validation loss\n    best_model_path = \"best_model.pth\"  # Path to save the best model\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for images, targets in train_loader:\n            images = images.to(device)\n            bboxes = [torch.tensor(t, dtype=torch.float32).to(device) for t in targets]  # List of bounding boxes\n            labels = [int(1) for t in targets]  # List of labels\n            labels = torch.tensor(labels, dtype=torch.float32).to(device)\n            preds_bbox, preds_label = model(images)\n            # Compute losses\n            bbox_losses = []\n            label_losses = []\n            for i in range(len(bboxes)):\n              bbox_losses.append(bbox_loss_fn(preds_bbox[i], bboxes[i]))\n              label_losses.append(label_loss_fn(preds_label[i], labels[i].unsqueeze(-1)))\n\n            bbox_loss = torch.mean(torch.stack(bbox_losses))\n            label_loss = torch.mean(torch.stack(label_losses))\n            loss = bbox_loss + label_loss\n\n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n\n        # Validate and save the best model\n        val_loss = validate_model(model, val_loader)\n        if val_loss < best_val_loss:\n            print(f\"Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. Saving model...\")\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), best_model_path)\n\n    print(\"Training complete. Best model saved as:\", best_model_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:20:37.856920Z","iopub.execute_input":"2025-04-03T15:20:37.857223Z","iopub.status.idle":"2025-04-03T15:20:37.873527Z","shell.execute_reply.started":"2025-04-03T15:20:37.857201Z","shell.execute_reply":"2025-04-03T15:20:37.872839Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def normalize_boxes(preds):\n    \"\"\"\n    Normalize the 'boxes' in the predictions to ensure they are all tensors of shape [N, 4].\n    Args:\n        preds: List of dictionaries with 'boxes' and 'labels'.\n    Returns:\n        Normalized predictions with 'boxes' as tensors of shape [N, 4].\n    \"\"\"\n    for pred in preds:\n        # If boxes is a list of tensors, stack them into a single tensor\n        if isinstance(pred['boxes'], list):\n            pred['boxes'] = torch.stack(pred['boxes'])  # Stack into [N, 4]\n    return preds","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-04-03T15:20:37.874091Z","iopub.execute_input":"2025-04-03T15:20:37.874336Z","iopub.status.idle":"2025-04-03T15:20:37.894342Z","shell.execute_reply.started":"2025-04-03T15:20:37.874317Z","shell.execute_reply":"2025-04-03T15:20:37.893566Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def validate_model(model, val_loader):\n    metric = IntersectionOverUnion().to(device)\n    model.eval()\n    total_bbox_loss = 0\n    total_label_loss = 0\n    total_iou = []\n    with torch.no_grad():\n        for images, targets in val_loader:\n            images = images.to(device)\n            bboxes = [torch.tensor(t, dtype=torch.float32).to(device) for t in targets]  # List of bounding boxes\n            labels = [int(1) for t in targets]  # List of labels\n            labels = torch.tensor(labels, dtype=torch.float32).to(device)\n            preds_bbox, preds_label = model(images)\n            # print('labels')\n            # print(preds_label)\n            # input()\n            bbox_losses = []\n            label_losses = []\n            # print(bboxes)\n            # print('//////////////////////////////')\n            for i in range(len(bboxes)):\n              bbox_losses.append(bbox_loss_fn(preds_bbox[i], bboxes[i]))\n              label_losses.append(label_loss_fn(preds_label[i], labels[i].unsqueeze(-1)))\n            #   print([bboxes[i]])\n            #   print('///////////////////////////////////////')\n            #   print(preds_bbox[i].shape)\n              preds = [\n                {\"boxes\": [preds_bbox[i]], \"labels\": preds_label[i]}\n                ]\n              preds = normalize_boxes(preds)\n            #   print(\"Preds\")\n            #   print(preds)\n              targets_combined = torch.cat([bboxes[i]], dim=0)\n                # print(targets_combined)\n              targets = [\n                {\"boxes\": targets_combined, \"labels\": torch.ones(len(targets_combined)).to(device)}\n                ]\n              iou_value = metric(preds, targets)\n              total_iou.append(iou_value['iou'].item())\n            #   targets = targets.to(device)\n            #   print(\"Targets\")\n            #   print(targets)\n\n            total_bbox_loss += torch.mean(torch.stack(bbox_losses))\n            total_label_loss += torch.mean(torch.stack(label_losses))\n            # print(targets)\n            # loss = total_label_loss + total_label_loss\n\n            # total_bbox_loss += bbox_loss_fn(preds_bbox, bboxes).item()\n            # total_label_loss += label_loss_fn(preds_label, labels).item()\n\n    # Calculate average validation loss\n    avg_bbox_loss = total_bbox_loss / len(val_loader)\n    avg_label_loss = total_label_loss / len(val_loader)\n    val_loss = avg_bbox_loss + avg_label_loss\n    print('IoU = ', sum(total_iou)/len(total_iou))\n    print(f\"Validation - BBox Loss: {avg_bbox_loss:.4f}, Label Loss: {avg_label_loss:.4f}, Total Loss: {val_loss:.4f}\")\n    return val_loss","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-04-03T15:20:37.895203Z","iopub.execute_input":"2025-04-03T15:20:37.895431Z","iopub.status.idle":"2025-04-03T15:20:37.914887Z","shell.execute_reply.started":"2025-04-03T15:20:37.895412Z","shell.execute_reply":"2025-04-03T15:20:37.914227Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# train\nbatch_size = BATCH_SIZE\ntarget_size = (224, 224)\ntrain_loader, val_loader = get_dataloaders(base_adress, annotations, batch_size, target_size)\ntrain_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:20:37.915644Z","iopub.execute_input":"2025-04-03T15:20:37.915935Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-9-fd23a653541d>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  bboxes = [torch.tensor(t, dtype=torch.float32).to(device) for t in targets]  # List of bounding boxes\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([2, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([1, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([3, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([6, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([5, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([4, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([10, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([15, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([22, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([7, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([8, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([13, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([9, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([12, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([11, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([27, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Loss: 2780.4451\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-11-3ae0e263f413>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  bboxes = [torch.tensor(t, dtype=torch.float32).to(device) for t in targets]  # List of bounding boxes\n","output_type":"stream"},{"name":"stdout","text":"IoU =  0.07150726919724856\nValidation - BBox Loss: 42.6263, Label Loss: 0.0000, Total Loss: 42.6263\nValidation loss improved from inf to 42.6263. Saving model...\nEpoch 2/3, Loss: 39.8367\nIoU =  0.27105190966306286\nValidation - BBox Loss: 29.9102, Label Loss: 0.0000, Total Loss: 29.9102\nValidation loss improved from 42.6263 to 29.9102. Saving model...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def calculate_iou(pred_box, gt_box):\n    \"\"\"\n    Calculate IoU (Intersection over Union) for a single pair of boxes.\n    Args:\n        pred_box: Tensor of shape (4,), [x_min, y_min, x_max, y_max].\n        gt_box: Tensor of shape (4,), [x_min, y_min, x_max, y_max].\n    Returns:\n        IoU value (float).\n    \"\"\"\n    # Determine the (x, y)-coordinates of the intersection rectangle\n    x1 = max(pred_box[0], gt_box[0])\n    y1 = max(pred_box[1], gt_box[1])\n    x2 = min(pred_box[2], gt_box[2])\n    y2 = min(pred_box[3], gt_box[3])\n\n    # Compute the area of intersection rectangle\n    inter_width = max(0, x2 - x1)\n    inter_height = max(0, y2 - y1)\n    inter_area = inter_width * inter_height\n\n    # Compute the area of both the predicted and ground-truth rectangles\n    pred_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n    gt_area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n\n    # Compute the area of union\n    union_area = pred_area + gt_area - inter_area\n\n    # Compute IoU\n    iou = inter_area / union_area if union_area > 0 else 0.0\n    return iou","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# You can write your function to evaluate your trained model\n> You can use calculate_iou function in you evaluation function","metadata":{}},{"cell_type":"markdown","source":"## Notes 03/29\n* what dataset are we meant to evaluate on?\n* next steps:\n    * ~get label loss to improve~ - it isnt affected in mobilenet either\n    * write evaluation function using calculate_iou\n    * issue where iou loss is suddenly shrinking (diverging?)\n    * top iou loss only 36 <- NEXT\n* changing the batch size from 32->64 has a big effect","metadata":{}},{"cell_type":"code","source":"import time\ndef evaluate(model, test_loader, device):\n    predictions = []\n    ious = []\n    inference_times = []\n\n    model.eval()\n    with torch.no_grad():\n        for images, targets in test_loader:\n            images = images.to(device)\n            bboxes = [torch.tensor(t, dtype=torch.float32).to(device) for t in targets]  # List of bounding boxes\n            labels = [int(1) for t in targets]  # List of labels\n            labels = torch.tensor(labels, dtype=torch.float32).to(device)\n\n            # get prediction\n            start_time = time.time()\n            pbbox, plabel = model(images)\n            end_time = time.time()\n            inference_times.append(end_time-start_time)\n\n            # format prediction\n            preds = [{\"boxes\": [pbbox, \"labels\": plabel}]\n            preds = normalize_boxes(preds)\n            predictions.append(preds)\n\n            if DEBUG:\n                print(pbbox)\n                print(bboxes)\n\n            # compare predicted box to all GT boxes\n            iou = -1\n            for i in range(len(bboxes)):\n                curr_iou = calculate_iou(pbbox, bboxes)  # TODO: un-tensor-fy\n                if curr_iou > iou:  # return prediction with highest IOU\n                    iou = curr_iou\n            ious.append(iou)\n    assert len(inference_times) > 10  # time for warm up and to get a good average\n    inference_times = inference_times[5:-1]\n    print('Average IoU = ', sum(ious)/len(ious))\n    print('Average inference time = ', sum(inference_times)/len(inference_times), 's')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# This is for the next part of the assignment","metadata":{}},{"cell_type":"markdown","source":"# Exporting to ONNX","metadata":{}},{"cell_type":"code","source":"#!pip install onnx\n#!pip install onnxscript","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n# Model class must be defined somewhere\nPATH = '/kaggle/working/best_model.pth'\nmodel = MobileNetV2FaceDetector().to(device)\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval()\ndummy_input = torch.randn(1, 3, 224, 224).to(device)  # Adjust shape based on your model's input size\n\n# Export the model to ONNX\ntorch.onnx.export(\n    model,  # The loaded PyTorch model\n    dummy_input,  # Example input tensor\n    \"model.onnx\",  # Output ONNX file name\n    export_params=True,  # Store trained parameters\n    opset_version=13,  # ONNX version (adjust as needed)\n    do_constant_folding=True,  # Optimize by folding constants\n    input_names=[\"input\"],  # Naming input tensor\n    output_names=[\"output\"],  # Naming output tensor\n    dynamic_axes=None \n)\n\nprint(\"Model successfully exported to ONNX!\")\n\"\"\"","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}
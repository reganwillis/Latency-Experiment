{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10984842,"sourceType":"datasetVersion","datasetId":6836738}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# packages\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import models, transforms\nfrom torchmetrics.detection import IntersectionOverUnion\nfrom PIL import Image\nimport os\nimport cv2\nimport numpy as np\nimport json\nimport shutil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:44:38.042585Z","iopub.execute_input":"2025-03-29T14:44:38.042863Z","iopub.status.idle":"2025-03-29T14:44:52.838506Z","shell.execute_reply.started":"2025-03-29T14:44:38.042817Z","shell.execute_reply":"2025-03-29T14:44:52.837550Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:44:52.839381Z","iopub.execute_input":"2025-03-29T14:44:52.839773Z","iopub.status.idle":"2025-03-29T14:44:52.926122Z","shell.execute_reply.started":"2025-03-29T14:44:52.839749Z","shell.execute_reply":"2025-03-29T14:44:52.925171Z"},"jupyter":{"source_hidden":true}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"#Data base addresses\nbase_adress = '/kaggle/input/dataset-face-detection-for-edge-computing-class/Dataset_FDDB/Dataset_FDDB/images'\nlabels_adr = '/kaggle/input/dataset-face-detection-for-edge-computing-class/Dataset_FDDB/Dataset_FDDB/label.txt'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:44:52.927119Z","iopub.execute_input":"2025-03-29T14:44:52.927526Z","iopub.status.idle":"2025-03-29T14:44:52.956127Z","shell.execute_reply.started":"2025-03-29T14:44:52.927474Z","shell.execute_reply":"2025-03-29T14:44:52.955421Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Make the labels ready\n\nwith open(labels_adr, 'r') as f:\n    lines = f.readlines()\nannotations = []\nbboxes = []\nflag = False\nfor line in lines:\n    if line.startswith('#'):\n      if flag:\n        annotations.append({'image':img_name, 'bboxes': bboxes})\n        bboxes = []\n      flag = True\n      img_name = line[2:]\n    else:\n      x_min, y_min, x_max, y_max = line.split()\n      bboxes.append([int(x_min), int(y_min), int(x_max), int(y_max)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:44:52.958338Z","iopub.execute_input":"2025-03-29T14:44:52.958636Z","iopub.status.idle":"2025-03-29T14:44:52.998129Z","shell.execute_reply.started":"2025-03-29T14:44:52.958614Z","shell.execute_reply":"2025-03-29T14:44:52.997257Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Custom Dataset Class for FDDB\nclass FDDBDataset(Dataset):\n    def __init__(self, img_dir, annot_file, target_size=(224, 224), transform=None):\n        self.img_dir = img_dir\n        self.target_size = target_size\n        self.transform = transform\n        self.data = self._parse_annotations(annot_file)\n\n    def _parse_annotations(self, annot_file):\n        \n        data = []\n        for el in annot_file:\n          img_path = os.path.join(self.img_dir, el['image'][:-1])\n          boxes = el['bboxes']\n          data.append((img_path, boxes))\n        return data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_path, boxes = self.data[idx]\n        image = cv2.imread(img_path)\n        if image is None:\n            raise FileNotFoundError(f\"Image not found: {img_path}\")\n\n        # Original dimensions\n        h, w, _ = image.shape\n\n        # Resize image\n        image_resized = cv2.resize(image, self.target_size)\n        target_h, target_w = self.target_size\n\n        # Scale bounding boxes\n        scale_x = target_w / w\n        scale_y = target_h / h\n        boxes_resized = []\n        for box in boxes:\n            x_min = int(box[0] * scale_x)\n            y_min = int(box[1] * scale_y)\n            x_max = int(box[2] * scale_x)\n            y_max = int(box[3] * scale_y)\n            boxes_resized.append([x_min, y_min, x_max, y_max])\n\n        # Convert to tensor\n        if self.transform:\n            image_resized = self.transform(image_resized)\n        else:\n            image_resized = transforms.ToTensor()(image_resized)\n\n        return image_resized, torch.tensor(boxes_resized, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:44:52.999355Z","iopub.execute_input":"2025-03-29T14:44:52.999612Z","iopub.status.idle":"2025-03-29T14:44:53.007220Z","shell.execute_reply.started":"2025-03-29T14:44:52.999593Z","shell.execute_reply":"2025-03-29T14:44:53.006400Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# DataLoader preparation\ndef get_dataloaders(img_dir, annot_file, batch_size=16, target_size=(224, 224), validation_split=0.2):\n\n    # Transformations\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    # Dataset\n    dataset = FDDBDataset(img_dir, annot_file, target_size, transform)\n\n    # Split dataset\n    val_size = int(len(dataset) * validation_split)\n    train_size = len(dataset) - val_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n    # DataLoaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\n    return train_loader, val_loader\n\ndef collate_fn(batch):\n    \"\"\"\n    Custom collate function to handle variable-length bounding box arrays.\n\n    :param batch: List of tuples (image, boxes).\n    :return: Tuple of images and targets.\n    \"\"\"\n    images = torch.stack([item[0] for item in batch])\n    targets = [item[1] for item in batch]\n    return images, targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:44:53.008082Z","iopub.execute_input":"2025-03-29T14:44:53.008309Z","iopub.status.idle":"2025-03-29T14:44:53.029942Z","shell.execute_reply.started":"2025-03-29T14:44:53.008290Z","shell.execute_reply":"2025-03-29T14:44:53.029049Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"># Network Architecture\n> **You need to change this architecture**\n># NOTE:\n> **You are not allowed to use pre-trained models**","metadata":{}},{"cell_type":"code","source":"from torchsummary import summary\n\nclass MyFaceDetector(nn.Module):\n    def __init__(self, pretrained=False):\n        super().__init__()\n        # TODO: resize input images to 448p (or stay 244p?)\n        # TODO: threshold the resulting detections by the model's confidence\n        # TODO: feature extractor backbone\n        # architecture from original YOLO paper\n\n        # conv layer 1\n        self.conv1 = nn.Conv2d(\n            in_channels=3,\n            out_channels=64,\n            kernel_size=7,\n            stride=2,\n            padding=1\n        )\n        self.relu1 = nn.LeakyReLU(negative_slope=0.1)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv2 = nn.Conv2d(\n            in_channels=64,\n            out_channels=192,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu2 = nn.LeakyReLU(negative_slope=0.1)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv3 = nn.Conv2d(\n            in_channels=192,\n            out_channels=128,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu3 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv4 = nn.Conv2d(\n            in_channels=128,\n            out_channels=256,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu4 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv5 = nn.Conv2d(\n            in_channels=256,\n            out_channels=256,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu5 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv6 = nn.Conv2d(\n            in_channels=256,\n            out_channels=512,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu6 = nn.LeakyReLU(negative_slope=0.1)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv7 = nn.Conv2d(\n            in_channels=512,\n            out_channels=256,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu7 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv8 = nn.Conv2d(\n            in_channels=256,\n            out_channels=512,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu8 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv9 = nn.Conv2d(\n            in_channels=512,\n            out_channels=256,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu9 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv10 = nn.Conv2d(\n            in_channels=256,\n            out_channels=512,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu10 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv11 = nn.Conv2d(\n            in_channels=512,\n            out_channels=256,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu11 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv12 = nn.Conv2d(\n            in_channels=256,\n            out_channels=512,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu12 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv13 = nn.Conv2d(\n            in_channels=512,\n            out_channels=256,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu13 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv14 = nn.Conv2d(\n            in_channels=256,\n            out_channels=512,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu14 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv15 = nn.Conv2d(\n            in_channels=512,\n            out_channels=512,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu15 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv16 = nn.Conv2d(\n            in_channels=512,\n            out_channels=1024,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu16 = nn.LeakyReLU(negative_slope=0.1)\n        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.conv17 = nn.Conv2d(\n            in_channels=1024,\n            out_channels=512,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu17 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv18 = nn.Conv2d(\n            in_channels=512,\n            out_channels=1024,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu18 = nn.LeakyReLU(negative_slope=0.1)\n        # TODO: is the order of multipliers from the architecture correct?\n        self.conv19 = nn.Conv2d(\n            in_channels=1024,\n            out_channels=512,\n            kernel_size=1,\n            stride=1,\n            padding=1\n        )\n        self.relu19 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv20 = nn.Conv2d(\n            in_channels=512,\n            out_channels=1024,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu20 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv21 = nn.Conv2d(\n            in_channels=1024,\n            out_channels=1024,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu21 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv22 = nn.Conv2d(\n            in_channels=1024,\n            out_channels=1024,\n            kernel_size=3,\n            stride=2,\n            padding=1\n        )\n        self.relu22 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv23 = nn.Conv2d(\n            in_channels=1024,\n            out_channels=1024,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu23 = nn.LeakyReLU(negative_slope=0.1)\n        self.conv24 = nn.Conv2d(\n            in_channels=1024,\n            out_channels=1024,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.relu24 = nn.LeakyReLU(negative_slope=0.1)\n\n        self.flatten = nn.Flatten()\n        self.linear = nn.Linear(4096, 1)\n        \n        # (w-k+2p)/s+1\n        #linear_layer_input_size = (1024-3+2)/1+1\n        #print(linear_layer_input_size)\n        \n        # regression head\n        self.bbox = nn.Sequential(\n            nn.Linear(1024*8*8, 4096),\n            # TODO: dropout layer as mentioned in paper\n            nn.LeakyReLU(negative_slope=0.1),\n            nn.Linear(4096, 4)\n        )\n        \n        # classification head\n        self.classify = nn.Sequential(\n            nn.Linear(1024*8*8, 4096),\n            # TODO: dropout layer as mentioned in paper\n            nn.LeakyReLU(negative_slope=0.1),\n            nn.Linear(4096, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        # pass through feature extractor backbone\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.maxpool1(x)\n        x = self.conv2(x)\n        x = self.relu2(x)\n        x = self.maxpool2(x)\n        x = self.conv3(x)\n        x = self.relu3(x)\n        x = self.conv4(x)\n        x = self.relu4(x)\n        x = self.conv5(x)\n        x = self.relu5(x)\n        x = self.conv6(x)\n        x = self.relu6(x)\n        x = self.maxpool3(x)\n        x = self.conv7(x)\n        x = self.relu7(x)\n        x = self.conv8(x)\n        x = self.relu8(x)\n        x = self.conv9(x)\n        x = self.relu9(x)\n        x = self.conv10(x)\n        x = self.relu10(x)\n        x = self.conv11(x)\n        x = self.relu11(x)\n        x = self.conv12(x)\n        x = self.relu12(x)\n        x = self.conv13(x)\n        x = self.relu13(x)\n        x = self.conv14(x)\n        x = self.relu14(x)\n        x = self.conv15(x)\n        x = self.relu15(x)\n        x = self.conv16(x)\n        x = self.relu16(x)\n        x = self.maxpool4(x)\n        x = self.conv17(x)\n        x = self.relu17(x)\n        x = self.conv18(x)\n        x = self.relu18(x)\n        x = self.conv19(x)\n        x = self.relu19(x)\n        x = self.conv20(x)\n        x = self.relu20(x)\n        x = self.conv21(x)\n        x = self.relu21(x)\n        x = self.conv22(x)\n        x = self.relu22(x)\n        x = self.conv23(x)\n        x = self.relu23(x)\n        x = self.conv24(x)\n        x = self.relu24(x)\n\n        #_, c, h, w = x.shape\n        #print(c, h, w)\n        \n        x = self.flatten(x)\n        #x = torch.flatten(x, 1)\n        #x = self.linear(x)\n        \n        bbox = self.bbox(x)\n        label = self.classify(x)\n\n        return bbox, label\nmodel = MyFaceDetector().to(device)\nsummary(model, (3, 224, 224))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:44:53.030949Z","iopub.execute_input":"2025-03-29T14:44:53.031269Z","iopub.status.idle":"2025-03-29T14:45:00.173209Z","shell.execute_reply.started":"2025-03-29T14:44:53.031239Z","shell.execute_reply":"2025-03-29T14:45:00.172261Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 110, 110]           9,472\n         LeakyReLU-2         [-1, 64, 110, 110]               0\n         MaxPool2d-3           [-1, 64, 55, 55]               0\n            Conv2d-4          [-1, 192, 55, 55]         110,784\n         LeakyReLU-5          [-1, 192, 55, 55]               0\n         MaxPool2d-6          [-1, 192, 27, 27]               0\n            Conv2d-7          [-1, 128, 29, 29]          24,704\n         LeakyReLU-8          [-1, 128, 29, 29]               0\n            Conv2d-9          [-1, 256, 29, 29]         295,168\n        LeakyReLU-10          [-1, 256, 29, 29]               0\n           Conv2d-11          [-1, 256, 31, 31]          65,792\n        LeakyReLU-12          [-1, 256, 31, 31]               0\n           Conv2d-13          [-1, 512, 31, 31]       1,180,160\n        LeakyReLU-14          [-1, 512, 31, 31]               0\n        MaxPool2d-15          [-1, 512, 15, 15]               0\n           Conv2d-16          [-1, 256, 17, 17]         131,328\n        LeakyReLU-17          [-1, 256, 17, 17]               0\n           Conv2d-18          [-1, 512, 17, 17]       1,180,160\n        LeakyReLU-19          [-1, 512, 17, 17]               0\n           Conv2d-20          [-1, 256, 19, 19]         131,328\n        LeakyReLU-21          [-1, 256, 19, 19]               0\n           Conv2d-22          [-1, 512, 19, 19]       1,180,160\n        LeakyReLU-23          [-1, 512, 19, 19]               0\n           Conv2d-24          [-1, 256, 21, 21]         131,328\n        LeakyReLU-25          [-1, 256, 21, 21]               0\n           Conv2d-26          [-1, 512, 21, 21]       1,180,160\n        LeakyReLU-27          [-1, 512, 21, 21]               0\n           Conv2d-28          [-1, 256, 23, 23]         131,328\n        LeakyReLU-29          [-1, 256, 23, 23]               0\n           Conv2d-30          [-1, 512, 23, 23]       1,180,160\n        LeakyReLU-31          [-1, 512, 23, 23]               0\n           Conv2d-32          [-1, 512, 25, 25]         262,656\n        LeakyReLU-33          [-1, 512, 25, 25]               0\n           Conv2d-34         [-1, 1024, 25, 25]       4,719,616\n        LeakyReLU-35         [-1, 1024, 25, 25]               0\n        MaxPool2d-36         [-1, 1024, 12, 12]               0\n           Conv2d-37          [-1, 512, 14, 14]         524,800\n        LeakyReLU-38          [-1, 512, 14, 14]               0\n           Conv2d-39         [-1, 1024, 14, 14]       4,719,616\n        LeakyReLU-40         [-1, 1024, 14, 14]               0\n           Conv2d-41          [-1, 512, 16, 16]         524,800\n        LeakyReLU-42          [-1, 512, 16, 16]               0\n           Conv2d-43         [-1, 1024, 16, 16]       4,719,616\n        LeakyReLU-44         [-1, 1024, 16, 16]               0\n           Conv2d-45         [-1, 1024, 16, 16]       9,438,208\n        LeakyReLU-46         [-1, 1024, 16, 16]               0\n           Conv2d-47           [-1, 1024, 8, 8]       9,438,208\n        LeakyReLU-48           [-1, 1024, 8, 8]               0\n           Conv2d-49           [-1, 1024, 8, 8]       9,438,208\n        LeakyReLU-50           [-1, 1024, 8, 8]               0\n           Conv2d-51           [-1, 1024, 8, 8]       9,438,208\n        LeakyReLU-52           [-1, 1024, 8, 8]               0\n          Flatten-53                [-1, 65536]               0\n           Linear-54                 [-1, 4096]     268,439,552\n        LeakyReLU-55                 [-1, 4096]               0\n           Linear-56                    [-1, 4]          16,388\n           Linear-57                 [-1, 4096]     268,439,552\n        LeakyReLU-58                 [-1, 4096]               0\n           Linear-59                    [-1, 1]           4,097\n          Sigmoid-60                    [-1, 1]               0\n================================================================\nTotal params: 597,055,557\nTrainable params: 597,055,557\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 93.27\nParams size (MB): 2277.59\nEstimated Total Size (MB): 2371.43\n----------------------------------------------------------------\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"class MobileNetV2FaceDetector(nn.Module):\n    def __init__(self, pretrained=True):\n        super(MobileNetV2FaceDetector, self).__init__()\n        # Load MobileNetV2 base\n        self.base = models.mobilenet_v2(pretrained=pretrained).features\n        self.pool = nn.AdaptiveAvgPool2d(1)\n\n        # Custom head for bounding box and classification\n        self.fc_bbox = nn.Sequential(\n            nn.Linear(1280, 512),\n            nn.ReLU(),\n            nn.Linear(512, 4),  # Bounding box: [x_min, y_min, x_max, y_max]\n        )\n        self.fc_label = nn.Sequential(\n            nn.Linear(1280, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1),  # Binary classification: face/no face\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        x = self.base(x)\n        x = self.pool(x).view(x.size(0), -1)\n        bbox = self.fc_bbox(x)\n        label = self.fc_label(x)\n        return bbox, label\n\n#model = MobileNetV2FaceDetector().to(device)\n# after 100 epochs, IoU was ~0.56","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:45:00.174026Z","iopub.execute_input":"2025-03-29T14:45:00.174239Z","iopub.status.idle":"2025-03-29T14:45:00.179771Z","shell.execute_reply.started":"2025-03-29T14:45:00.174220Z","shell.execute_reply":"2025-03-29T14:45:00.178919Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# loss / optimizer / train\n# Loss functions\nbbox_loss_fn = nn.SmoothL1Loss()  # For bounding box regression\nlabel_loss_fn = nn.BCELoss()      # For binary classification\n\n# Optimizer\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# Training loop\ndef train_model(model, train_loader, val_loader, num_epochs=10):\n    best_val_loss = float('inf')  # Initialize best validation loss\n    best_model_path = \"best_model.pth\"  # Path to save the best model\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for images, targets in train_loader:\n            images = images.to(device)\n            bboxes = [torch.tensor(t, dtype=torch.float32).to(device) for t in targets]  # List of bounding boxes\n            labels = [int(1) for t in targets]  # List of labels\n            labels = torch.tensor(labels, dtype=torch.float32).to(device)\n            preds_bbox, preds_label = model(images)\n            # Compute losses\n            bbox_losses = []\n            label_losses = []\n            for i in range(len(bboxes)):\n              bbox_losses.append(bbox_loss_fn(preds_bbox[i], bboxes[i]))\n              label_losses.append(label_loss_fn(preds_label[i], labels[i].unsqueeze(-1)))\n\n            bbox_loss = torch.mean(torch.stack(bbox_losses))\n            label_loss = torch.mean(torch.stack(label_losses))\n            loss = bbox_loss + label_loss\n\n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n\n        # Validate and save the best model\n        val_loss = validate_model(model, val_loader)\n        if val_loss < best_val_loss:\n            print(f\"Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. Saving model...\")\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), best_model_path)\n\n    print(\"Training complete. Best model saved as:\", best_model_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:45:00.180534Z","iopub.execute_input":"2025-03-29T14:45:00.180755Z","iopub.status.idle":"2025-03-29T14:45:00.203591Z","shell.execute_reply.started":"2025-03-29T14:45:00.180738Z","shell.execute_reply":"2025-03-29T14:45:00.202937Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def normalize_boxes(preds):\n    \"\"\"\n    Normalize the 'boxes' in the predictions to ensure they are all tensors of shape [N, 4].\n    Args:\n        preds: List of dictionaries with 'boxes' and 'labels'.\n    Returns:\n        Normalized predictions with 'boxes' as tensors of shape [N, 4].\n    \"\"\"\n    for pred in preds:\n        # If boxes is a list of tensors, stack them into a single tensor\n        if isinstance(pred['boxes'], list):\n            pred['boxes'] = torch.stack(pred['boxes'])  # Stack into [N, 4]\n    return preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:45:00.204310Z","iopub.execute_input":"2025-03-29T14:45:00.204601Z","iopub.status.idle":"2025-03-29T14:45:00.223811Z","shell.execute_reply.started":"2025-03-29T14:45:00.204580Z","shell.execute_reply":"2025-03-29T14:45:00.223113Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def validate_model(model, val_loader):\n    metric = IntersectionOverUnion().to(device)\n    model.eval()\n    total_bbox_loss = 0\n    total_label_loss = 0\n    total_iou = []\n    with torch.no_grad():\n        for images, targets in val_loader:\n            images = images.to(device)\n            bboxes = [torch.tensor(t, dtype=torch.float32).to(device) for t in targets]  # List of bounding boxes\n            labels = [int(1) for t in targets]  # List of labels\n            labels = torch.tensor(labels, dtype=torch.float32).to(device)\n            preds_bbox, preds_label = model(images)\n            # print('labels')\n            # print(preds_label)\n            # input()\n            bbox_losses = []\n            label_losses = []\n            # print(bboxes)\n            # print('//////////////////////////////')\n            for i in range(len(bboxes)):\n              bbox_losses.append(bbox_loss_fn(preds_bbox[i], bboxes[i]))\n              label_losses.append(label_loss_fn(preds_label[i], labels[i].unsqueeze(-1)))\n            #   print([bboxes[i]])\n            #   print('///////////////////////////////////////')\n            #   print(preds_bbox[i].shape)\n              preds = [\n                {\"boxes\": [preds_bbox[i]], \"labels\": preds_label[i]}\n                ]\n              preds = normalize_boxes(preds)\n            #   print(\"Preds\")\n            #   print(preds)\n              targets_combined = torch.cat([bboxes[i]], dim=0)\n                # print(targets_combined)\n              targets = [\n                {\"boxes\": targets_combined, \"labels\": torch.ones(len(targets_combined)).to(device)}\n                ]\n              iou_value = metric(preds, targets)\n              total_iou.append(iou_value['iou'].item())\n            #   targets = targets.to(device)\n            #   print(\"Targets\")\n            #   print(targets)\n\n            total_bbox_loss += torch.mean(torch.stack(bbox_losses))\n            total_label_loss += torch.mean(torch.stack(label_losses))\n            # print(targets)\n            # loss = total_label_loss + total_label_loss\n\n            # total_bbox_loss += bbox_loss_fn(preds_bbox, bboxes).item()\n            # total_label_loss += label_loss_fn(preds_label, labels).item()\n\n    # Calculate average validation loss\n    avg_bbox_loss = total_bbox_loss / len(val_loader)\n    avg_label_loss = total_label_loss / len(val_loader)\n    val_loss = avg_bbox_loss + avg_label_loss\n    print('IoU = ', sum(total_iou)/len(total_iou))\n    print(f\"Validation - BBox Loss: {avg_bbox_loss:.4f}, Label Loss: {avg_label_loss:.4f}, Total Loss: {val_loss:.4f}\")\n    return val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:45:00.224598Z","iopub.execute_input":"2025-03-29T14:45:00.224802Z","iopub.status.idle":"2025-03-29T14:45:00.240431Z","shell.execute_reply.started":"2025-03-29T14:45:00.224784Z","shell.execute_reply":"2025-03-29T14:45:00.239798Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# train\nbatch_size = 32\ntarget_size = (224, 224)\ntrain_loader, val_loader = get_dataloaders(base_adress, annotations, batch_size, target_size)\ntrain_model(model, train_loader, val_loader, num_epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:45:00.241238Z","iopub.execute_input":"2025-03-29T14:45:00.241547Z","iopub.status.idle":"2025-03-29T14:50:31.225944Z","shell.execute_reply.started":"2025-03-29T14:45:00.241519Z","shell.execute_reply":"2025-03-29T14:50:31.224917Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-9-9f95b738bbcf>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  bboxes = [torch.tensor(t, dtype=torch.float32).to(device) for t in targets]  # List of bounding boxes\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([1, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([2, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([5, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([4, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([7, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([3, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([8, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([15, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([9, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([13, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([10, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([27, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([6, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([11, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([22, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Loss: 19667034.2841\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-11-3ae0e263f413>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  bboxes = [torch.tensor(t, dtype=torch.float32).to(device) for t in targets]  # List of bounding boxes\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1040: UserWarning: Using a target size (torch.Size([12, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n","output_type":"stream"},{"name":"stdout","text":"IoU =  0.0\nValidation - BBox Loss: 4181522.7500, Label Loss: 100.0000, Total Loss: 4181622.7500\nValidation loss improved from inf to 4181622.7500. Saving model...\nEpoch 2/5, Loss: 29001948557.9366\nIoU =  0.0\nValidation - BBox Loss: 274537664.0000, Label Loss: 100.0000, Total Loss: 274537760.0000\nEpoch 3/5, Loss: 143842620.6338\nIoU =  0.0\nValidation - BBox Loss: 3808403.2500, Label Loss: 100.0000, Total Loss: 3808503.2500\nValidation loss improved from 4181622.7500 to 3808503.2500. Saving model...\nEpoch 4/5, Loss: 1813690.9780\nIoU =  0.0\nValidation - BBox Loss: 924771.8125, Label Loss: 100.0000, Total Loss: 924871.8125\nValidation loss improved from 3808503.2500 to 924871.8125. Saving model...\nEpoch 5/5, Loss: 761294.7232\nIoU =  0.0\nValidation - BBox Loss: 557056.6875, Label Loss: 100.0000, Total Loss: 557156.6875\nValidation loss improved from 924871.8125 to 557156.6875. Saving model...\nTraining complete. Best model saved as: best_model.pth\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def calculate_iou(pred_box, gt_box):\n    \"\"\"\n    Calculate IoU (Intersection over Union) for a single pair of boxes.\n    Args:\n        pred_box: Tensor of shape (4,), [x_min, y_min, x_max, y_max].\n        gt_box: Tensor of shape (4,), [x_min, y_min, x_max, y_max].\n    Returns:\n        IoU value (float).\n    \"\"\"\n    # Determine the (x, y)-coordinates of the intersection rectangle\n    x1 = max(pred_box[0], gt_box[0])\n    y1 = max(pred_box[1], gt_box[1])\n    x2 = min(pred_box[2], gt_box[2])\n    y2 = min(pred_box[3], gt_box[3])\n\n    # Compute the area of intersection rectangle\n    inter_width = max(0, x2 - x1)\n    inter_height = max(0, y2 - y1)\n    inter_area = inter_width * inter_height\n\n    # Compute the area of both the predicted and ground-truth rectangles\n    pred_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n    gt_area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n\n    # Compute the area of union\n    union_area = pred_area + gt_area - inter_area\n\n    # Compute IoU\n    iou = inter_area / union_area if union_area > 0 else 0.0\n    return iou","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-03-29T14:50:31.230599Z","iopub.execute_input":"2025-03-29T14:50:31.231005Z","iopub.status.idle":"2025-03-29T14:50:31.240532Z","shell.execute_reply.started":"2025-03-29T14:50:31.230964Z","shell.execute_reply":"2025-03-29T14:50:31.239654Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# You can write your function to evaluate your trained model\n> You can use calculate_iou function in you evaluation function","metadata":{}},{"cell_type":"code","source":"def evaluate():\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:50:31.241713Z","iopub.execute_input":"2025-03-29T14:50:31.242010Z","iopub.status.idle":"2025-03-29T14:50:31.263699Z","shell.execute_reply.started":"2025-03-29T14:50:31.241982Z","shell.execute_reply":"2025-03-29T14:50:31.262713Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# This is for the next part of the assignment","metadata":{}},{"cell_type":"markdown","source":"# Exporting to ONNX","metadata":{}},{"cell_type":"code","source":"#!pip install onnx\n#!pip install onnxscript","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:50:31.264733Z","iopub.execute_input":"2025-03-29T14:50:31.265087Z","iopub.status.idle":"2025-03-29T14:50:31.281968Z","shell.execute_reply.started":"2025-03-29T14:50:31.265043Z","shell.execute_reply":"2025-03-29T14:50:31.281333Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"\"\"\"\n# Model class must be defined somewhere\nPATH = '/kaggle/working/best_model.pth'\nmodel = MobileNetV2FaceDetector().to(device)\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval()\ndummy_input = torch.randn(1, 3, 224, 224).to(device)  # Adjust shape based on your model's input size\n\n# Export the model to ONNX\ntorch.onnx.export(\n    model,  # The loaded PyTorch model\n    dummy_input,  # Example input tensor\n    \"model.onnx\",  # Output ONNX file name\n    export_params=True,  # Store trained parameters\n    opset_version=13,  # ONNX version (adjust as needed)\n    do_constant_folding=True,  # Optimize by folding constants\n    input_names=[\"input\"],  # Naming input tensor\n    output_names=[\"output\"],  # Naming output tensor\n    dynamic_axes=None \n)\n\nprint(\"Model successfully exported to ONNX!\")\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:50:31.282782Z","iopub.execute_input":"2025-03-29T14:50:31.283056Z","iopub.status.idle":"2025-03-29T14:50:31.300825Z","shell.execute_reply.started":"2025-03-29T14:50:31.283030Z","shell.execute_reply":"2025-03-29T14:50:31.299987Z"},"jupyter":{"source_hidden":true}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'\\n# Model class must be defined somewhere\\nPATH = \\'/kaggle/working/best_model.pth\\'\\nmodel = MobileNetV2FaceDetector().to(device)\\nmodel.load_state_dict(torch.load(PATH))\\nmodel.eval()\\ndummy_input = torch.randn(1, 3, 224, 224).to(device)  # Adjust shape based on your model\\'s input size\\n\\n# Export the model to ONNX\\ntorch.onnx.export(\\n    model,  # The loaded PyTorch model\\n    dummy_input,  # Example input tensor\\n    \"model.onnx\",  # Output ONNX file name\\n    export_params=True,  # Store trained parameters\\n    opset_version=13,  # ONNX version (adjust as needed)\\n    do_constant_folding=True,  # Optimize by folding constants\\n    input_names=[\"input\"],  # Naming input tensor\\n    output_names=[\"output\"],  # Naming output tensor\\n    dynamic_axes=None \\n)\\n\\nprint(\"Model successfully exported to ONNX!\")\\n'"},"metadata":{}}],"execution_count":16}]}